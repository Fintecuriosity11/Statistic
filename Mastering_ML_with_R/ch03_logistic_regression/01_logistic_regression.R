##############################################################################################################

#(주의) ->  순차적으로 코드를 실행하는 것을 권함!

##############################################################################################################




##############################################################################################################

### 모형화와 평가
## 이 과정에서는 먼저 모든 입력 변수로 로지스틱 모형을 만든 후 점차 줄여 나가서 최량 부분 집합을 생성.
## 그 후에는 판별분석(discriminant analysis)과 다변량 적응 회귀 스플라인(Multivariate Adaptive Regression Splines, Mars)시도.


### 로지스틱 회귀 모형(logistic regression model).
## 모형을 적합화(fitting)하는 단계

##############################################################################################################


#glm()함수는 로지스틱 회귀를 포함하는 일군의 일반화된 선형모형(generalized linear regression models)을 모아놓음
#lm()함수와 비슷하지만, 한가지 크게 다른 점은 family=binomial라는 인자(argument)를 함수 안에 쓴다.
#이렇게 함으로 R에게 다른 버전의 일반화된 선형 모형 대신 로지스틱 회귀 방법을 사용하라고 알려줌.

#훈련세트 안에 있는 모든 피처를 포함하는 모형을 만들고, 테스트 셋을 이용해 성능 체크

##############################################################################################################


full.fit<-glm(class ~., family = binomial, data = train)
summary(full.fit)

#summary()함수를 쓰면 계수와 해당 p-value(p값)을 점검할 수 있음.
#출력물을 확인하면 단 2개의 피처(thinkness, nuclei)만 0.05보다 작은 값을 갖음

confint(full.fit)

#confint()함수로 다음과 같이 95% 신로구간을 불러올 수 있음

##############################################################################################################


#가장 유의하게 나타난 두 피처(thick, nucl)는 구간 속에 0을 포함하지 않는다는 것에 주목해야함.
#로지스틱 회귀에서는 피처의 계수를 X(독립변수, 피처)가 한 단위 변화할 때 Y(종속 변수)가 변화하는 양을 나타낸다
#해석 불가능

#이 시점에서 odds비가 많은 도움이 되는데 로그함수에서 beta라는 계수는 오즈비 e^beta로 변환 가능

##############################################################################################################

#오즈비 생성하려면 exp(coef())라는 syntax 사용.

exp(coef(full.fit))

##############################################################################################################


#오즈비는 피처가 한 단위 변했을 때 나타나는 결과의 오즈(odds, 비율비)로 해석 가능함.


#만일, 이 값이 1보다 크면, 피처가 증가할 때 결과의 오즈도 증가함.
#이와 반대로, 1보다 작으면 피처가 증가할 때 결과의 오즈는 감소함.

#예제에서는 u.size를 제외한 모든 피처가 로그 오즈(log odds)를 증가시킴.
#이전에 데이터를 탐색할 때 다중 공선성이 있을 가능성 있다고 했음
#선형회귀에서 했던 것처럼, 로지스틱 회귀를 이용해 VIF(분산 팽창 인자)통계량을 계산

##############################################################################################################


library(car)

vif(full.fit)

#결과 값들 중 어느 것도 VIF에 관한 경험 법칙(rule of thumb) 통계값인 5보다 크지 않으므로 공선성의 문제가 없다.

#피처 선택 단계
#모형이 train 및 test 데이터에 얼마나 잘 작동하는지 확인
#예측된 확률들로 벡터 생성.

train.prob <- predict(full.fit, type = "response")
train.prob[1:5] #처음 5개의 예측 변수만 확인.


#훈련 데이터(training data)로 모형의 성능을 평가하고 모형이 얼마나 test set에 적합한지 평가.

#혼동행렬 생성(confusion matrix) 

#여기에서는 결과값이 0이거나 1이 되도록 만들어야 함.
#함수가 양성또는 음성을 고르는 기본값은 0.5인데, 여기서 확률값이 0.5보다 높으면 악성으로 분류



install.packages("InformationValue")
library(InformationValue)

trainY <- y[ind==1]
testY <- y[ind==2]

confusionMatrix(trainY, train.prob)

##############################################################################################################

#결과에서 행은 예측값, 열은 실제값을 나타남.
#대각선에서는 올바르게 분류된 개수가 나타남.

#상단 우측의 7은 허위 음성, 좌하단의 8은 허위 양성을 나타냄.

##############################################################################################################

#또한 오류 비율도 아래와 같이 계산 가능.

misClassError(trainY, train.prob)

#훈련(training set)에 관해서는 오류 비율이 3.16%(0.0316)에 불과한 것이 상당히 분류를 잘한 것으로 판단.
#아직 관찰하지 않은 값에 관해서도 정확하게 분류할 수 있어야 함.
#즉 test datat에 관해서도 성능이 좋아야 함.

##############################################################################################################

#test data(테스트 데이터.)에 관해서도 오류 행렬을 생성하는 것도 training data(훈련 데이터.) 경우와 비슷함.

test.probs <- predict(full.fit, newdata = test, type = "response")
misClassError(testY, test.probs)

confusionMatrix(testY, test.probs)

#오류 비율이 약 2.4%(0.0239) 나오는데 2%의 오류을 100%에 빼서 98%의 예측률로 산정하였음.
#결과 확인 후 모든 피처를 포함한 분류모형이 98%의 예측률 기록.

##############################################################################################################
